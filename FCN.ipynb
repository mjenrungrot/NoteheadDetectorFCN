{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:30.578923Z",
     "start_time": "2018-06-13T22:21:29.471859Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirlab/miniconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Activation, Reshape, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import add\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "import os\n",
    "import glob\n",
    "from random import shuffle, randint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:30.583548Z",
     "start_time": "2018-06-13T22:21:30.580397Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def resize_images(x, size, method='bilinear'):\n",
    "    new_size = tf.convert_to_tensor(size, dtype=tf.int32)\n",
    "    resized = tf.image.resize_images(x, new_size)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:30.806418Z",
     "start_time": "2018-06-13T22:21:30.585090Z"
    },
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class BilinearUpSampling2D(Layer):\n",
    "    \"\"\"Upsampling2D with bilinear interpolation.\"\"\"\n",
    "\n",
    "    def __init__(self, target_shape=None, data_format=None, **kwargs):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        assert data_format in {\n",
    "            'channels_last', 'channels_first'}\n",
    "        self.data_format = data_format\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.target_shape = target_shape\n",
    "        if self.data_format == 'channels_first':\n",
    "            self.target_size = (target_shape[2], target_shape[3])\n",
    "        elif self.data_format == 'channels_last':\n",
    "            self.target_size = (target_shape[1], target_shape[2])\n",
    "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], self.target_size[0],\n",
    "                    self.target_size[1], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1],\n",
    "                    self.target_size[0], self.target_size[1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return resize_images(inputs, size=self.target_size,\n",
    "                             method='bilinear')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class CroppingLike2D(Layer):\n",
    "    def __init__(self, target_shape, offset=None, data_format=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"Crop to target.\n",
    "        If only one `offset` is set, then all dimensions are offset by this amount.\n",
    "        \"\"\"\n",
    "        super(CroppingLike2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.target_shape = target_shape\n",
    "        if offset is None or offset == 'centered':\n",
    "            self.offset = 'centered'\n",
    "        elif isinstance(offset, int):\n",
    "            self.offset = (offset, offset)\n",
    "        elif hasattr(offset, '__len__'):\n",
    "            if len(offset) != 2:\n",
    "                raise ValueError('`offset` should have two elements. '\n",
    "                                 'Found: ' + str(offset))\n",
    "            self.offset = offset\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    self.target_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0],\n",
    "                    self.target_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if self.data_format == 'channels_first':\n",
    "            input_height = input_shape[2]\n",
    "            input_width = input_shape[3]\n",
    "            target_height = self.target_shape[2]\n",
    "            target_width = self.target_shape[3]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "\n",
    "            return inputs[:,\n",
    "                          :,\n",
    "                          self.offset[0]:self.offset[0] + target_height,\n",
    "                          self.offset[1]:self.offset[1] + target_width]\n",
    "        elif self.data_format == 'channels_last':\n",
    "            input_height = input_shape[1]\n",
    "            input_width = input_shape[2]\n",
    "            target_height = self.target_shape[1]\n",
    "            target_width = self.target_shape[2]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "            output = inputs[:,\n",
    "                            self.offset[0]:self.offset[0] + target_height,\n",
    "                            self.offset[1]:self.offset[1] + target_width,\n",
    "                            :]\n",
    "            return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                  'offset': self.offset,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(CroppingLike2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:30.935665Z",
     "start_time": "2018-06-13T22:21:30.807987Z"
    },
    "code_folding": [
     0,
     86
    ]
   },
   "outputs": [],
   "source": [
    "def vgg_conv(filters, convs, padding=False, weight_decay=0.,\n",
    "             block_name='blockx'):\n",
    "    def f(x):\n",
    "        for i in range(convs):\n",
    "            if block_name == 'block1' and i == 0:\n",
    "                if padding is True:\n",
    "                    x = ZeroPadding2D(padding=(100, 100))(x)\n",
    "                x = Conv2D(filters, (3,3), activation='relu', padding='same',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=l2(weight_decay),\n",
    "                           name='{}_conv{}'.format(block_name, int(i+1)))(x)\n",
    "            else:\n",
    "                x = Conv2D(filters, (3,3), activation='relu', padding='same',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=l2(weight_decay),\n",
    "                           name='{}_conv{}'.format(block_name, int(i+1)))(x)\n",
    "        pool = MaxPooling2D((2,2), strides=(2,2), padding='same',\n",
    "                            name='{}_pool'.format(block_name))(x)\n",
    "        return pool\n",
    "    return f\n",
    "\n",
    "def vgg_fc(filters, weight_decay=0., block_name='block5'):\n",
    "    def f(x):\n",
    "        fc6 = Conv2D(filters=4096, kernel_size=(3, 3),\n",
    "                     activation='relu', padding='same',\n",
    "                     dilation_rate=(2, 2),\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(weight_decay),\n",
    "                     name='{}_fc6'.format(block_name))(x)\n",
    "        drop6 = Dropout(0.5)(fc6)\n",
    "        return drop6\n",
    "    return f\n",
    "\n",
    "def vgg_deconv(classes, scale=1, kernel_size=(4, 4), strides=(2, 2),\n",
    "               crop_offset='centered', weight_decay=0., block_name='featx'):\n",
    "    def f(x, y):\n",
    "        def scaling(xx, ss=1):\n",
    "            return xx * ss\n",
    "        scaled = Lambda(scaling, arguments={'ss': scale},\n",
    "                        name='scale_{}'.format(block_name))(x)\n",
    "        score = Conv2D(filters=classes, kernel_size=(1, 1),\n",
    "                       activation='linear',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(weight_decay),\n",
    "                       name='score_{}'.format(block_name))(scaled)\n",
    "        if y is None:\n",
    "            upscore = Conv2DTranspose(filters=classes, kernel_size=kernel_size,\n",
    "                                      strides=strides, padding='valid',\n",
    "                                      kernel_initializer='he_normal',\n",
    "                                      kernel_regularizer=l2(weight_decay),\n",
    "                                      use_bias=False,\n",
    "                                      name='upscore_{}'.format(block_name))(score)\n",
    "        else:\n",
    "            crop = CroppingLike2D(target_shape=K.int_shape(y),\n",
    "                                  offset=crop_offset,\n",
    "                                  name='crop_{}'.format(block_name))(score)\n",
    "            merge = add([y, crop])\n",
    "            upscore = Conv2DTranspose(filters=classes, kernel_size=kernel_size,\n",
    "                                      strides=strides, padding='valid',\n",
    "                                      kernel_initializer='he_normal',\n",
    "                                      kernel_regularizer=l2(weight_decay),\n",
    "                                      use_bias=False,\n",
    "                                      name='upscore_{}'.format(block_name))(merge)\n",
    "        return upscore\n",
    "    return f\n",
    "\n",
    "def vgg_upsampling(classes, target_shape=None, scale=1, weight_decay=0., block_name='featx'):\n",
    "    def f(x, y):\n",
    "        score = Conv2D(filters=classes, kernel_size=(1, 1),\n",
    "                       activation='linear',\n",
    "                       padding='valid',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(weight_decay),\n",
    "                       name='score_{}'.format(block_name))(x)\n",
    "        if y is not None:\n",
    "            def scaling(xx, ss=1):\n",
    "                return xx * ss\n",
    "            scaled = Lambda(scaling, arguments={'ss': scale},\n",
    "                            name='scale_{}'.format(block_name))(score)\n",
    "            score = add([y, scaled])\n",
    "        upscore = BilinearUpSampling2D(\n",
    "            target_shape=target_shape,\n",
    "            name='upscore_{}'.format(block_name))(score)\n",
    "        return upscore\n",
    "    return f\n",
    "\n",
    "def vgg_score(crop_offset='centered'):\n",
    "    def f(x, y):\n",
    "        score = CroppingLike2D(target_shape=K.int_shape(\n",
    "            x), offset=crop_offset, name='score')(y)\n",
    "        return score\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:30.992253Z",
     "start_time": "2018-06-13T22:21:30.937040Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, inputs, blocks, \n",
    "                 weights=None, trainable=True,\n",
    "                 name='encoder'):\n",
    "        inverse_pyramid = []\n",
    "        \n",
    "        conv_blocks = blocks[:-1]\n",
    "        for i, block in enumerate(conv_blocks):\n",
    "            if i == 0:\n",
    "                x = block(inputs)\n",
    "                inverse_pyramid.append(x)\n",
    "            elif i < len(conv_blocks) - 1:\n",
    "                x = block(x)\n",
    "                inverse_pyramid.append(x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        \n",
    "        # fully convolutional block\n",
    "        fc_block = blocks[-1]\n",
    "        y = fc_block(x)\n",
    "        inverse_pyramid.append(y)\n",
    "        \n",
    "        # Reverse the pyramid features\n",
    "        outputs = list(reversed(inverse_pyramid))\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.067158Z",
     "start_time": "2018-06-13T22:21:30.994442Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Decoder(pyramid, blocks):\n",
    "    if len(blocks) != len(pyramid):\n",
    "        raise ValueError('`blocks` needs to match the length of'\n",
    "                         '`pyramid`.')\n",
    "    \n",
    "    decoded = None\n",
    "    for feat, blk in zip(pyramid, blocks):\n",
    "        decoded = blk(feat, decoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.136038Z",
     "start_time": "2018-06-13T22:21:31.069276Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(250, 250, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.170208Z",
     "start_time": "2018-06-13T22:21:31.137564Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = [vgg_conv(64, 1, block_name='block1'),\n",
    "          vgg_conv(128, 1, block_name='block2'),\n",
    "          vgg_conv(256, 1, block_name='block3'),\n",
    "          vgg_conv(512, 1, block_name='block4'),\n",
    "          vgg_conv(512, 1, block_name='block5'),\n",
    "          vgg_fc(4096)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.380280Z",
     "start_time": "2018-06-13T22:21:31.172960Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(inputs, blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.384395Z",
     "start_time": "2018-06-13T22:21:31.381746Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_pyramid = encoder.outputs   # A feature pyramid with 5 scales\n",
    "feat_pyramid = feat_pyramid[:4]  # Select only the top three scale of the pyramid\n",
    "feat_pyramid.append(inputs)      # Add image to the bottom of the pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.540920Z",
     "start_time": "2018-06-13T22:21:31.385630Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_blocks = [\n",
    "vgg_upsampling(classes=124, target_shape=(None, 16, 16, None), scale=1, block_name='feat1'),            \n",
    "vgg_upsampling(classes=124, target_shape=(None, 32, 32, None),  scale=0.01, block_name='feat2'),\n",
    "vgg_upsampling(classes=124, target_shape=(None, 63, 63, None),  scale=0.0001, block_name='feat3'),\n",
    "vgg_upsampling(classes=124, target_shape=(None, 250, 250, None),  scale=0.0001, block_name='feat4'),\n",
    "]\n",
    "\n",
    "outputs = Decoder(pyramid=feat_pyramid[:-1], blocks=decode_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.552314Z",
     "start_time": "2018-06-13T22:21:31.542394Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = Activation('softmax')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.594423Z",
     "start_time": "2018-06-13T22:21:31.553653Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.726608Z",
     "start_time": "2018-06-13T22:21:31.595902Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:31.730527Z",
     "start_time": "2018-06-13T22:21:31.728275Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:32.103171Z",
     "start_time": "2018-06-13T22:21:31.731996Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class seg_dataset_reader:\n",
    "    path = \"\"\n",
    "    class_mappings = \"\"\n",
    "    files = []\n",
    "    images = []\n",
    "    annotations = []\n",
    "    test_images = []\n",
    "    test_annotations = []\n",
    "    batch_offset = 0\n",
    "    epochs_completed = 0\n",
    "\n",
    "    def __init__(self, deepscores_path, max_pages=40, crop=True, crop_size=[1000,1000], test_size=20):\n",
    "        \"\"\"\n",
    "        Initialize a file reader for the DeepScores classification data\n",
    "        :param records_list: path to the dataset\n",
    "        sample record: {'image': f, 'annotation': annotation_file, 'filename': filename}\n",
    "        \"\"\"\n",
    "        print(\"Initializing DeepScores Classification Batch Dataset Reader...\")\n",
    "        self.path = deepscores_path\n",
    "        self.max_pages = max_pages\n",
    "        self.crop = crop\n",
    "        self.crop_size = crop_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "        images_list = []\n",
    "        images_glob = os.path.join(self.path, \"images_png\", '*.' + 'png')\n",
    "        images_list.extend(glob.glob(images_glob))\n",
    "\n",
    "        #shuffle image list\n",
    "        shuffle(images_list)\n",
    "\n",
    "        if max_pages is None:\n",
    "            max_pages = len(images_list)\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        if max_pages > len(images_list):\n",
    "            print(\"Not enough data, only \" + str(len(images_list)) + \" available\")\n",
    "\n",
    "        if test_size >= max_pages:\n",
    "            print(\"Test set too big (\"+str(test_size)+\"), max_pages is: \"+str(max_pages))\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        print(\"Splitting dataset, train: \"+str(max_pages-test_size)+\" images, test: \"+str(test_size)+ \" images\")\n",
    "        test_image_list = images_list[0:test_size]\n",
    "        train_image_list = images_list[test_size:max_pages]\n",
    "\n",
    "        # test_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in test_image_list]\n",
    "        # train_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in train_image_list]\n",
    "\n",
    "        self._read_images(test_image_list,train_image_list)\n",
    "\n",
    "    def _read_images(self,test_image_list,train_image_list):\n",
    "\n",
    "        dat_train = [self._transform(filename) for filename in train_image_list]\n",
    "        for dat in dat_train:\n",
    "            self.images.append(dat[0])\n",
    "            self.annotations.append(dat[1])\n",
    "        self.images = np.array(self.images)\n",
    "        self.images = np.expand_dims(self.images, -1)\n",
    "\n",
    "        self.annotations = np.array(self.annotations)\n",
    "        self.annotations = np.expand_dims(self.annotations, -1)\n",
    "\n",
    "        print(\"Training set done\")\n",
    "        dat_test = [self._transform(filename) for filename in test_image_list]\n",
    "        for dat in dat_test:\n",
    "            self.test_images.append(dat[0])\n",
    "            self.test_annotations.append(dat[1])\n",
    "        self.test_images = np.array(self.test_images)\n",
    "        self.test_images = np.expand_dims(self.test_images, -1)\n",
    "\n",
    "        self.test_annotations = np.array(self.test_annotations)\n",
    "        self.test_annotations = np.expand_dims(self.test_annotations, -1)\n",
    "        print(\"Test set done\")\n",
    "\n",
    "\n",
    "    def _transform(self, filename):\n",
    "        image = misc.imread(filename)\n",
    "        annotation = misc.imread(filename.replace(\"/images_png/\", \"/pix_annotations_png/\"))\n",
    "        print(\"im working!\" + str(randint(0,10)))\n",
    "        if not image.shape[0:2] == annotation.shape[0:2]:\n",
    "            print(\"input and annotation have different sizes!\")\n",
    "            import sys\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            sys.exit(1)\n",
    "\n",
    "        if image.shape[-1] != 1:\n",
    "            # take mean over color channels, image BW anyways --> fix in dataset creation\n",
    "            image = np.mean(image, -1)\n",
    "\n",
    "        if self.crop:\n",
    "            coord_0 = randint(0, (image.shape[0] - self.crop_size[0]))\n",
    "            coord_1 = randint(0, (image.shape[1] - self.crop_size[1]))\n",
    "\n",
    "            image = image[coord_0:(coord_0+self.crop_size[0]),coord_1:(coord_1+self.crop_size[1])]\n",
    "            annotation = annotation[coord_0:(coord_0 + self.crop_size[0]), coord_1:(coord_1 + self.crop_size[1])]\n",
    "\n",
    "        return [image, annotation]\n",
    "\n",
    "    # from PIL import Image\n",
    "    # im = Image.fromarray(image)\n",
    "    # im.show()\n",
    "    # im = Image.fromarray(annotation)\n",
    "    # im.show()\n",
    "\n",
    "\n",
    "    def get_records(self):\n",
    "        return self.images, self.annotations\n",
    "\n",
    "    def reset_batch_offset(self, offset=0):\n",
    "        self.batch_offset = offset\n",
    "\n",
    "    def get_test_records(self):\n",
    "        return self.test_images, self.test_annotations\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.batch_offset\n",
    "        self.batch_offset += batch_size\n",
    "        if self.batch_offset > self.images.shape[0]:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            print(\"****************** Epochs completed: \" + str(self.epochs_completed) + \"******************\")\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.images.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            self.images = self.images[perm]\n",
    "            self.annotations = self.annotations[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.batch_offset = batch_size\n",
    "\n",
    "        end = self.batch_offset\n",
    "        return self.images[start:end], self.annotations[start:end]\n",
    "\n",
    "    def get_random_batch(self, batch_size):\n",
    "        indexes = np.random.randint(0, self.images.shape[0], size=[batch_size]).tolist()\n",
    "        return self.images[indexes], self.annotations[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:49.851069Z",
     "start_time": "2018-06-13T22:21:32.104510Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeepScores Classification Batch Dataset Reader...\n",
      "Splitting dataset, train: 20 images, test: 20 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirlab/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:80: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/mirlab/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:81: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im working!3\n",
      "im working!6\n",
      "im working!6\n",
      "im working!2\n",
      "im working!6\n",
      "im working!9\n",
      "im working!6\n",
      "im working!5\n",
      "im working!0\n",
      "im working!1\n",
      "im working!1\n",
      "im working!7\n",
      "im working!4\n",
      "im working!4\n",
      "im working!10\n",
      "im working!9\n",
      "im working!0\n",
      "im working!2\n",
      "im working!1\n",
      "im working!7\n",
      "Training set done\n",
      "im working!0\n",
      "im working!5\n",
      "im working!3\n",
      "im working!5\n",
      "im working!0\n",
      "im working!3\n",
      "im working!5\n",
      "im working!10\n",
      "im working!6\n",
      "im working!10\n",
      "im working!2\n",
      "im working!10\n",
      "im working!4\n",
      "im working!8\n",
      "im working!6\n",
      "im working!6\n",
      "im working!4\n",
      "im working!1\n",
      "im working!3\n",
      "im working!7\n",
      "Test set done\n"
     ]
    }
   ],
   "source": [
    "data_reader = seg_dataset_reader('/home/mirlab/Downloads/DeepScores/',\n",
    "                                 crop=True, crop_size=[250,250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:49.854082Z",
     "start_time": "2018-06-13T22:21:49.852256Z"
    }
   },
   "outputs": [],
   "source": [
    "train_img, train_annotation = data_reader.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:52.674603Z",
     "start_time": "2018-06-13T22:21:49.855071Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 2s 2s/step - loss: 16.1181 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3c47fb1128>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_img, y=train_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:52.757644Z",
     "start_time": "2018-06-13T22:21:52.676185Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(model.predict(train_img), axis=-1) # get the argmax of 124 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:21:52.910739Z",
     "start_time": "2018-06-13T22:21:52.758940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Semantic Prediction')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3c442e33c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_img[0,:,:,0], cmap='gray')\n",
    "plt.title(\"Input image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(prediction[0,:,:], cmap='gray')\n",
    "plt.title(\"Semantic Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
