{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:16:37.416140Z",
     "start_time": "2018-06-13T22:16:36.990982Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Activation, Reshape, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import add\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "import os\n",
    "import glob\n",
    "from random import shuffle, randint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:39.694728Z",
     "start_time": "2018-06-13T22:12:39.691047Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def resize_images(x, size, method='bilinear'):\n",
    "    new_size = tf.convert_to_tensor(size, dtype=tf.int32)\n",
    "    resized = tf.image.resize_images(x, new_size)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.021677Z",
     "start_time": "2018-06-13T22:12:39.766302Z"
    },
    "code_folding": [
     0,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class BilinearUpSampling2D(Layer):\n",
    "    \"\"\"Upsampling2D with bilinear interpolation.\"\"\"\n",
    "\n",
    "    def __init__(self, target_shape=None, data_format=None, **kwargs):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        assert data_format in {\n",
    "            'channels_last', 'channels_first'}\n",
    "        self.data_format = data_format\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.target_shape = target_shape\n",
    "        if self.data_format == 'channels_first':\n",
    "            self.target_size = (target_shape[2], target_shape[3])\n",
    "        elif self.data_format == 'channels_last':\n",
    "            self.target_size = (target_shape[1], target_shape[2])\n",
    "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], self.target_size[0],\n",
    "                    self.target_size[1], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1],\n",
    "                    self.target_size[0], self.target_size[1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return resize_images(inputs, size=self.target_size,\n",
    "                             method='bilinear')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class CroppingLike2D(Layer):\n",
    "    def __init__(self, target_shape, offset=None, data_format=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"Crop to target.\n",
    "        If only one `offset` is set, then all dimensions are offset by this amount.\n",
    "        \"\"\"\n",
    "        super(CroppingLike2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.target_shape = target_shape\n",
    "        if offset is None or offset == 'centered':\n",
    "            self.offset = 'centered'\n",
    "        elif isinstance(offset, int):\n",
    "            self.offset = (offset, offset)\n",
    "        elif hasattr(offset, '__len__'):\n",
    "            if len(offset) != 2:\n",
    "                raise ValueError('`offset` should have two elements. '\n",
    "                                 'Found: ' + str(offset))\n",
    "            self.offset = offset\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    self.target_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0],\n",
    "                    self.target_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if self.data_format == 'channels_first':\n",
    "            input_height = input_shape[2]\n",
    "            input_width = input_shape[3]\n",
    "            target_height = self.target_shape[2]\n",
    "            target_width = self.target_shape[3]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "\n",
    "            return inputs[:,\n",
    "                          :,\n",
    "                          self.offset[0]:self.offset[0] + target_height,\n",
    "                          self.offset[1]:self.offset[1] + target_width]\n",
    "        elif self.data_format == 'channels_last':\n",
    "            input_height = input_shape[1]\n",
    "            input_width = input_shape[2]\n",
    "            target_height = self.target_shape[1]\n",
    "            target_width = self.target_shape[2]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "            output = inputs[:,\n",
    "                            self.offset[0]:self.offset[0] + target_height,\n",
    "                            self.offset[1]:self.offset[1] + target_width,\n",
    "                            :]\n",
    "            return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                  'offset': self.offset,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(CroppingLike2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.165644Z",
     "start_time": "2018-06-13T22:12:40.023293Z"
    },
    "code_folding": [
     0,
     86
    ]
   },
   "outputs": [],
   "source": [
    "def vgg_conv(filters, convs, padding=False, weight_decay=0.,\n",
    "             block_name='blockx'):\n",
    "    def f(x):\n",
    "        for i in range(convs):\n",
    "            if block_name == 'block1' and i == 0:\n",
    "                if padding is True:\n",
    "                    x = ZeroPadding2D(padding=(100, 100))(x)\n",
    "                x = Conv2D(filters, (3,3), activation='relu', padding='same',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=l2(weight_decay),\n",
    "                           name='{}_conv{}'.format(block_name, int(i+1)))(x)\n",
    "            else:\n",
    "                x = Conv2D(filters, (3,3), activation='relu', padding='same',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=l2(weight_decay),\n",
    "                           name='{}_conv{}'.format(block_name, int(i+1)))(x)\n",
    "        pool = MaxPooling2D((2,2), strides=(2,2), padding='same',\n",
    "                            name='{}_pool'.format(block_name))(x)\n",
    "        return pool\n",
    "    return f\n",
    "\n",
    "def vgg_fc(filters, weight_decay=0., block_name='block5'):\n",
    "    def f(x):\n",
    "        fc6 = Conv2D(filters=4096, kernel_size=(3, 3),\n",
    "                     activation='relu', padding='same',\n",
    "                     dilation_rate=(2, 2),\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(weight_decay),\n",
    "                     name='{}_fc6'.format(block_name))(x)\n",
    "        drop6 = Dropout(0.5)(fc6)\n",
    "        return drop6\n",
    "    return f\n",
    "\n",
    "def vgg_deconv(classes, scale=1, kernel_size=(4, 4), strides=(2, 2),\n",
    "               crop_offset='centered', weight_decay=0., block_name='featx'):\n",
    "    def f(x, y):\n",
    "        def scaling(xx, ss=1):\n",
    "            return xx * ss\n",
    "        scaled = Lambda(scaling, arguments={'ss': scale},\n",
    "                        name='scale_{}'.format(block_name))(x)\n",
    "        score = Conv2D(filters=classes, kernel_size=(1, 1),\n",
    "                       activation='linear',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(weight_decay),\n",
    "                       name='score_{}'.format(block_name))(scaled)\n",
    "        if y is None:\n",
    "            upscore = Conv2DTranspose(filters=classes, kernel_size=kernel_size,\n",
    "                                      strides=strides, padding='valid',\n",
    "                                      kernel_initializer='he_normal',\n",
    "                                      kernel_regularizer=l2(weight_decay),\n",
    "                                      use_bias=False,\n",
    "                                      name='upscore_{}'.format(block_name))(score)\n",
    "        else:\n",
    "            crop = CroppingLike2D(target_shape=K.int_shape(y),\n",
    "                                  offset=crop_offset,\n",
    "                                  name='crop_{}'.format(block_name))(score)\n",
    "            merge = add([y, crop])\n",
    "            upscore = Conv2DTranspose(filters=classes, kernel_size=kernel_size,\n",
    "                                      strides=strides, padding='valid',\n",
    "                                      kernel_initializer='he_normal',\n",
    "                                      kernel_regularizer=l2(weight_decay),\n",
    "                                      use_bias=False,\n",
    "                                      name='upscore_{}'.format(block_name))(merge)\n",
    "        return upscore\n",
    "    return f\n",
    "\n",
    "def vgg_upsampling(classes, target_shape=None, scale=1, weight_decay=0., block_name='featx'):\n",
    "    def f(x, y):\n",
    "        score = Conv2D(filters=classes, kernel_size=(1, 1),\n",
    "                       activation='linear',\n",
    "                       padding='valid',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(weight_decay),\n",
    "                       name='score_{}'.format(block_name))(x)\n",
    "        if y is not None:\n",
    "            def scaling(xx, ss=1):\n",
    "                return xx * ss\n",
    "            scaled = Lambda(scaling, arguments={'ss': scale},\n",
    "                            name='scale_{}'.format(block_name))(score)\n",
    "            score = add([y, scaled])\n",
    "        upscore = BilinearUpSampling2D(\n",
    "            target_shape=target_shape,\n",
    "            name='upscore_{}'.format(block_name))(score)\n",
    "        return upscore\n",
    "    return f\n",
    "\n",
    "def vgg_score(crop_offset='centered'):\n",
    "    def f(x, y):\n",
    "        score = CroppingLike2D(target_shape=K.int_shape(\n",
    "            x), offset=crop_offset, name='score')(y)\n",
    "        return score\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.329034Z",
     "start_time": "2018-06-13T22:12:40.307516Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, inputs, blocks, \n",
    "                 weights=None, trainable=True,\n",
    "                 name='encoder'):\n",
    "        inverse_pyramid = []\n",
    "        \n",
    "        conv_blocks = blocks[:-1]\n",
    "        for i, block in enumerate(conv_blocks):\n",
    "            if i == 0:\n",
    "                x = block(inputs)\n",
    "                inverse_pyramid.append(x)\n",
    "            elif i < len(conv_blocks) - 1:\n",
    "                x = block(x)\n",
    "                inverse_pyramid.append(x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        \n",
    "        # fully convolutional block\n",
    "        fc_block = blocks[-1]\n",
    "        y = fc_block(x)\n",
    "        inverse_pyramid.append(y)\n",
    "        \n",
    "        # Reverse the pyramid features\n",
    "        outputs = list(reversed(inverse_pyramid))\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.423025Z",
     "start_time": "2018-06-13T22:12:40.405445Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Decoder(pyramid, blocks):\n",
    "    if len(blocks) != len(pyramid):\n",
    "        raise ValueError('`blocks` needs to match the length of'\n",
    "                         '`pyramid`.')\n",
    "    \n",
    "    decoded = None\n",
    "    for feat, blk in zip(pyramid, blocks):\n",
    "        decoded = blk(feat, decoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.531384Z",
     "start_time": "2018-06-13T22:12:40.506299Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(250, 250, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.605452Z",
     "start_time": "2018-06-13T22:12:40.602235Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = [vgg_conv(64, 1, block_name='block1'),\n",
    "          vgg_conv(128, 1, block_name='block2'),\n",
    "          vgg_conv(256, 1, block_name='block3'),\n",
    "          vgg_conv(512, 1, block_name='block4'),\n",
    "          vgg_conv(512, 1, block_name='block5'),\n",
    "          vgg_fc(4096)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.856614Z",
     "start_time": "2018-06-13T22:12:40.680144Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(inputs, blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:40.940157Z",
     "start_time": "2018-06-13T22:12:40.937573Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_pyramid = encoder.outputs   # A feature pyramid with 5 scales\n",
    "feat_pyramid = feat_pyramid[:4]  # Select only the top three scale of the pyramid\n",
    "feat_pyramid.append(inputs)      # Add image to the bottom of the pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:41.210370Z",
     "start_time": "2018-06-13T22:12:41.103624Z"
    }
   },
   "outputs": [],
   "source": [
    "decode_blocks = [\n",
    "vgg_upsampling(classes=124, target_shape=(None, 16, 16, None), scale=1, block_name='feat1'),            \n",
    "vgg_upsampling(classes=124, target_shape=(None, 32, 32, None),  scale=0.01, block_name='feat2'),\n",
    "vgg_upsampling(classes=124, target_shape=(None, 63, 63, None),  scale=0.0001, block_name='feat3'),\n",
    "vgg_upsampling(classes=124, target_shape=(None, 250, 250, None),  scale=0.0001, block_name='feat4'),\n",
    "]\n",
    "\n",
    "outputs = Decoder(pyramid=feat_pyramid[:-1], blocks=decode_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:41.296818Z",
     "start_time": "2018-06-13T22:12:41.286580Z"
    }
   },
   "outputs": [],
   "source": [
    "outputs = Activation('softmax')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:41.376202Z",
     "start_time": "2018-06-13T22:12:41.373735Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:41.532269Z",
     "start_time": "2018-06-13T22:12:41.457242Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:18:58.408297Z",
     "start_time": "2018-06-13T22:18:58.404122Z"
    }
   },
   "outputs": [],
   "source": [
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:12:47.423129Z",
     "start_time": "2018-06-13T22:12:47.039951Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class seg_dataset_reader:\n",
    "    path = \"\"\n",
    "    class_mappings = \"\"\n",
    "    files = []\n",
    "    images = []\n",
    "    annotations = []\n",
    "    test_images = []\n",
    "    test_annotations = []\n",
    "    batch_offset = 0\n",
    "    epochs_completed = 0\n",
    "\n",
    "    def __init__(self, deepscores_path, max_pages=40, crop=True, crop_size=[1000,1000], test_size=20):\n",
    "        \"\"\"\n",
    "        Initialize a file reader for the DeepScores classification data\n",
    "        :param records_list: path to the dataset\n",
    "        sample record: {'image': f, 'annotation': annotation_file, 'filename': filename}\n",
    "        \"\"\"\n",
    "        print(\"Initializing DeepScores Classification Batch Dataset Reader...\")\n",
    "        self.path = deepscores_path\n",
    "        self.max_pages = max_pages\n",
    "        self.crop = crop\n",
    "        self.crop_size = crop_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "        images_list = []\n",
    "        images_glob = os.path.join(self.path, \"images_png\", '*.' + 'png')\n",
    "        images_list.extend(glob.glob(images_glob))\n",
    "\n",
    "        #shuffle image list\n",
    "        shuffle(images_list)\n",
    "\n",
    "        if max_pages is None:\n",
    "            max_pages = len(images_list)\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        if max_pages > len(images_list):\n",
    "            print(\"Not enough data, only \" + str(len(images_list)) + \" available\")\n",
    "\n",
    "        if test_size >= max_pages:\n",
    "            print(\"Test set too big (\"+str(test_size)+\"), max_pages is: \"+str(max_pages))\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        print(\"Splitting dataset, train: \"+str(max_pages-test_size)+\" images, test: \"+str(test_size)+ \" images\")\n",
    "        test_image_list = images_list[0:test_size]\n",
    "        train_image_list = images_list[test_size:max_pages]\n",
    "\n",
    "        # test_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in test_image_list]\n",
    "        # train_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in train_image_list]\n",
    "\n",
    "        self._read_images(test_image_list,train_image_list)\n",
    "\n",
    "    def _read_images(self,test_image_list,train_image_list):\n",
    "\n",
    "        dat_train = [self._transform(filename) for filename in train_image_list]\n",
    "        for dat in dat_train:\n",
    "            self.images.append(dat[0])\n",
    "            self.annotations.append(dat[1])\n",
    "        self.images = np.array(self.images)\n",
    "        self.images = np.expand_dims(self.images, -1)\n",
    "\n",
    "        self.annotations = np.array(self.annotations)\n",
    "        self.annotations = np.expand_dims(self.annotations, -1)\n",
    "\n",
    "        print(\"Training set done\")\n",
    "        dat_test = [self._transform(filename) for filename in test_image_list]\n",
    "        for dat in dat_test:\n",
    "            self.test_images.append(dat[0])\n",
    "            self.test_annotations.append(dat[1])\n",
    "        self.test_images = np.array(self.test_images)\n",
    "        self.test_images = np.expand_dims(self.test_images, -1)\n",
    "\n",
    "        self.test_annotations = np.array(self.test_annotations)\n",
    "        self.test_annotations = np.expand_dims(self.test_annotations, -1)\n",
    "        print(\"Test set done\")\n",
    "\n",
    "\n",
    "    def _transform(self, filename):\n",
    "        image = misc.imread(filename)\n",
    "        annotation = misc.imread(filename.replace(\"/images_png/\", \"/pix_annotations_png/\"))\n",
    "        print(\"im working!\" + str(randint(0,10)))\n",
    "        if not image.shape[0:2] == annotation.shape[0:2]:\n",
    "            print(\"input and annotation have different sizes!\")\n",
    "            import sys\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            sys.exit(1)\n",
    "\n",
    "        if image.shape[-1] != 1:\n",
    "            # take mean over color channels, image BW anyways --> fix in dataset creation\n",
    "            image = np.mean(image, -1)\n",
    "\n",
    "        if self.crop:\n",
    "            coord_0 = randint(0, (image.shape[0] - self.crop_size[0]))\n",
    "            coord_1 = randint(0, (image.shape[1] - self.crop_size[1]))\n",
    "\n",
    "            image = image[coord_0:(coord_0+self.crop_size[0]),coord_1:(coord_1+self.crop_size[1])]\n",
    "            annotation = annotation[coord_0:(coord_0 + self.crop_size[0]), coord_1:(coord_1 + self.crop_size[1])]\n",
    "\n",
    "        return [image, annotation]\n",
    "\n",
    "    # from PIL import Image\n",
    "    # im = Image.fromarray(image)\n",
    "    # im.show()\n",
    "    # im = Image.fromarray(annotation)\n",
    "    # im.show()\n",
    "\n",
    "\n",
    "    def get_records(self):\n",
    "        return self.images, self.annotations\n",
    "\n",
    "    def reset_batch_offset(self, offset=0):\n",
    "        self.batch_offset = offset\n",
    "\n",
    "    def get_test_records(self):\n",
    "        return self.test_images, self.test_annotations\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.batch_offset\n",
    "        self.batch_offset += batch_size\n",
    "        if self.batch_offset > self.images.shape[0]:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            print(\"****************** Epochs completed: \" + str(self.epochs_completed) + \"******************\")\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.images.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            self.images = self.images[perm]\n",
    "            self.annotations = self.annotations[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.batch_offset = batch_size\n",
    "\n",
    "        end = self.batch_offset\n",
    "        return self.images[start:end], self.annotations[start:end]\n",
    "\n",
    "    def get_random_batch(self, batch_size):\n",
    "        indexes = np.random.randint(0, self.images.shape[0], size=[batch_size]).tolist()\n",
    "        return self.images[indexes], self.annotations[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:13:05.936835Z",
     "start_time": "2018-06-13T22:12:47.433057Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing DeepScores Classification Batch Dataset Reader...\n",
      "Splitting dataset, train: 20 images, test: 20 images\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mirlab/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:80: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "/home/mirlab/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:81: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im working!1\n",
      "im working!4\n",
      "im working!7\n",
      "im working!10\n",
      "im working!1\n",
      "im working!2\n",
      "im working!7\n",
      "im working!8\n",
      "im working!6\n",
      "im working!10\n",
      "im working!4\n",
      "im working!4\n",
      "im working!0\n",
      "im working!5\n",
      "im working!7\n",
      "im working!8\n",
      "im working!9\n",
      "im working!10\n",
      "im working!1\n",
      "im working!6\n",
      "Training set done\n",
      "im working!9\n",
      "im working!4\n",
      "im working!9\n",
      "im working!0\n",
      "im working!3\n",
      "im working!3\n",
      "im working!10\n",
      "im working!2\n",
      "im working!6\n",
      "im working!8\n",
      "im working!10\n",
      "im working!10\n",
      "im working!7\n",
      "im working!1\n",
      "im working!0\n",
      "im working!2\n",
      "im working!5\n",
      "im working!7\n",
      "im working!10\n",
      "im working!7\n",
      "Test set done\n"
     ]
    }
   ],
   "source": [
    "data_reader = seg_dataset_reader('/home/mirlab/Downloads/DeepScores/',\n",
    "                                 crop=True, crop_size=[250,250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:13:06.080698Z",
     "start_time": "2018-06-13T22:13:06.078777Z"
    }
   },
   "outputs": [],
   "source": [
    "train_img, train_annotation = data_reader.next_batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:13:11.379433Z",
     "start_time": "2018-06-13T22:13:06.169446Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "1/1 [==============================] - 4s 4s/step - loss: 16.1181 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f32f5596a20>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=train_img, y=train_annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:16:52.419079Z",
     "start_time": "2018-06-13T22:16:52.380572Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(model.predict(train_img), axis=-1) # get the argmax of 124 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T22:18:44.093799Z",
     "start_time": "2018-06-13T22:18:43.913583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Semantic Prediction')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f32f54d5eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.imshow(train_img[0,:,:,0], cmap='gray')\n",
    "plt.title(\"Input image\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(prediction[0,:,:], cmap='gray')\n",
    "plt.title(\"Semantic Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
