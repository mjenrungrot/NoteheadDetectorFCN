{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:00:19.823672Z",
     "start_time": "2018-06-18T19:00:19.799849Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Flatten, Activation, Reshape, Dropout, Lambda\n",
    "from keras.layers.convolutional import Conv2D, Conv2DTranspose, MaxPooling2D, ZeroPadding2D\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.merge import add, concatenate\n",
    "from keras.engine.topology import Layer\n",
    "from keras.engine import InputSpec\n",
    "from keras.utils import conv_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from utils.BilinearUpSampling import BilinearUpSampling2D\n",
    "\n",
    "import numpy as np\n",
    "import scipy.misc as misc\n",
    "import os\n",
    "import glob\n",
    "from random import shuffle, randint\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:52.445249Z",
     "start_time": "2018-06-18T18:57:52.440136Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def resize_images(x, size, method='bilinear'):\n",
    "    new_size = tf.convert_to_tensor(size, dtype=tf.int32)\n",
    "    resized = tf.image.resize_images(x, new_size)\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:56.517403Z",
     "start_time": "2018-06-18T18:57:56.050899Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BilinearUpSampling2D(Layer):\n",
    "    \"\"\"Upsampling2D with bilinear interpolation.\"\"\"\n",
    "\n",
    "    def __init__(self, target_shape=None, data_format=None, **kwargs):\n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "        assert data_format in {\n",
    "            'channels_last', 'channels_first'}\n",
    "        self.data_format = data_format\n",
    "        self.input_spec = [InputSpec(ndim=4)]\n",
    "        self.target_shape = target_shape\n",
    "        if self.data_format == 'channels_first':\n",
    "            self.target_size = (target_shape[2], target_shape[3])\n",
    "        elif self.data_format == 'channels_last':\n",
    "            self.target_size = (target_shape[1], target_shape[2])\n",
    "        super(BilinearUpSampling2D, self).__init__(**kwargs)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], self.target_size[0],\n",
    "                    self.target_size[1], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1],\n",
    "                    self.target_size[0], self.target_size[1])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return resize_images(inputs, size=self.target_size,\n",
    "                             method='bilinear')\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                'data_format': self.data_format}\n",
    "        base_config = super(BilinearUpSampling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class CroppingLike2D(Layer):\n",
    "    def __init__(self, target_shape, offset=None, data_format=None,\n",
    "                 **kwargs):\n",
    "        \"\"\"Crop to target.\n",
    "        If only one `offset` is set, then all dimensions are offset by this amount.\n",
    "        \"\"\"\n",
    "        super(CroppingLike2D, self).__init__(**kwargs)\n",
    "        self.data_format = conv_utils.normalize_data_format(data_format)\n",
    "        self.target_shape = target_shape\n",
    "        if offset is None or offset == 'centered':\n",
    "            self.offset = 'centered'\n",
    "        elif isinstance(offset, int):\n",
    "            self.offset = (offset, offset)\n",
    "        elif hasattr(offset, '__len__'):\n",
    "            if len(offset) != 2:\n",
    "                raise ValueError('`offset` should have two elements. '\n",
    "                                 'Found: ' + str(offset))\n",
    "            self.offset = offset\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_first':\n",
    "            return (input_shape[0],\n",
    "                    input_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    self.target_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0],\n",
    "                    self.target_shape[1],\n",
    "                    self.target_shape[2],\n",
    "                    input_shape[3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = K.int_shape(inputs)\n",
    "        if self.data_format == 'channels_first':\n",
    "            input_height = input_shape[2]\n",
    "            input_width = input_shape[3]\n",
    "            target_height = self.target_shape[2]\n",
    "            target_width = self.target_shape[3]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "\n",
    "            return inputs[:,\n",
    "                          :,\n",
    "                          self.offset[0]:self.offset[0] + target_height,\n",
    "                          self.offset[1]:self.offset[1] + target_width]\n",
    "        elif self.data_format == 'channels_last':\n",
    "            input_height = input_shape[1]\n",
    "            input_width = input_shape[2]\n",
    "            target_height = self.target_shape[1]\n",
    "            target_width = self.target_shape[2]\n",
    "            if target_height > input_height or target_width > input_width:\n",
    "                raise ValueError('The Tensor to be cropped need to be smaller'\n",
    "                                 'or equal to the target Tensor.')\n",
    "\n",
    "            if self.offset == 'centered':\n",
    "                self.offset = [int((input_height - target_height) / 2),\n",
    "                               int((input_width - target_width) / 2)]\n",
    "\n",
    "            if self.offset[0] + target_height > input_height:\n",
    "                raise ValueError('Height index out of range: '\n",
    "                                 + str(self.offset[0] + target_height))\n",
    "            if self.offset[1] + target_width > input_width:\n",
    "                raise ValueError('Width index out of range:'\n",
    "                                 + str(self.offset[1] + target_width))\n",
    "            output = inputs[:,\n",
    "                            self.offset[0]:self.offset[0] + target_height,\n",
    "                            self.offset[1]:self.offset[1] + target_width,\n",
    "                            :]\n",
    "            return output\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'target_shape': self.target_shape,\n",
    "                  'offset': self.offset,\n",
    "                  'data_format': self.data_format}\n",
    "        base_config = super(CroppingLike2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:58.208034Z",
     "start_time": "2018-06-18T18:57:57.888541Z"
    },
    "code_folding": [
     86
    ]
   },
   "outputs": [],
   "source": [
    "def vgg_conv(filters, convs, padding=False, weight_decay=0.,\n",
    "             block_name='blockx'):\n",
    "    def f(x):\n",
    "        for i in range(convs):\n",
    "            if block_name == 'block1' and i == 0:\n",
    "                if padding is True:\n",
    "                    x = ZeroPadding2D(padding=(100, 100))(x)\n",
    "                x = Conv2D(filters, (3,3), activation='relu', padding='same',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=l2(weight_decay),\n",
    "                           name='{}_conv{}'.format(block_name, int(i+1)))(x)\n",
    "            else:\n",
    "                x = Conv2D(filters, (3,3), activation='relu', padding='same',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=l2(weight_decay),\n",
    "                           name='{}_conv{}'.format(block_name, int(i+1)))(x)\n",
    "        pool = MaxPooling2D((2,2), strides=(2,2), padding='same',\n",
    "                            name='{}_pool'.format(block_name))(x)\n",
    "        dropout = Dropout(0.85)(pool)\n",
    "        return dropout\n",
    "    return f\n",
    "\n",
    "def vgg_fc(filters, weight_decay=0., block_name='block5'):\n",
    "    def f(x):\n",
    "        fc6 = Conv2D(filters=4096, kernel_size=(3, 3),\n",
    "                     activation='relu', padding='same',\n",
    "                     dilation_rate=(2, 2),\n",
    "                     kernel_initializer='he_normal',\n",
    "                     kernel_regularizer=l2(weight_decay),\n",
    "                     name='{}_fc6'.format(block_name))(x)\n",
    "        drop6 = Dropout(0.85)(fc6)\n",
    "        return drop6\n",
    "    return f\n",
    "\n",
    "def vgg_deconv(classes, scale=1, kernel_size=(4, 4), strides=(2, 2), target_shape=None,\n",
    "               crop_offset='centered', weight_decay=0., block_name='featx'):\n",
    "    def f(x, y):\n",
    "        def scaling(xx, ss=1):\n",
    "            return xx * ss\n",
    "        scaled = Lambda(scaling, arguments={'ss': scale},\n",
    "                        name='scale_{}'.format(block_name))(x)\n",
    "        if y is None:\n",
    "            upscore = Conv2DTranspose(filters=classes, kernel_size=kernel_size,\n",
    "                                      strides=strides, padding='same',\n",
    "                                      kernel_initializer='he_normal',\n",
    "                                      kernel_regularizer=l2(weight_decay),\n",
    "                                      use_bias=True,\n",
    "                                      name='upscore_{}'.format(block_name))(scaled)\n",
    "            upsample = BilinearUpSampling2D(target_shape=target_shape,\n",
    "                                            name='upsample_{}'.format(block_name))(upscore)\n",
    "        else:\n",
    "            crop = CroppingLike2D(target_shape=K.int_shape(y),\n",
    "                                 name='crop_{}'.format(block_name))(scaled)\n",
    "            merge = concatenate([y, crop])\n",
    "    \n",
    "            fuse1 = Conv2D(filters=2*classes, kernel_size=(1,1), \n",
    "                           activation='relu', name='upscore_{}_fuse_1'.format(block_name))(merge)\n",
    "            fuse2 = Conv2D(filters=classes, kernel_size=(1,1), \n",
    "                           activation='relu', name='upscore_{}_fuse_2'.format(block_name))(fuse1)\n",
    "            upscore = Conv2DTranspose(filters=classes, kernel_size=kernel_size,\n",
    "                                      strides=strides, padding='same',\n",
    "                                      kernel_initializer='he_normal',\n",
    "                                      kernel_regularizer=l2(weight_decay),\n",
    "                                      use_bias=True,\n",
    "                                      name='upscore_{}'.format(block_name))(fuse2)\n",
    "            upsample = BilinearUpSampling2D(target_shape=target_shape,\n",
    "                                            name='upsample_{}'.format(block_name))(upscore)\n",
    "        return upsample\n",
    "    return f\n",
    "\n",
    "def vgg_upsampling(classes, target_shape=None, scale=1, weight_decay=0., block_name='featx'):\n",
    "    def f(x, y):\n",
    "        score = Conv2D(filters=classes, kernel_size=(1, 1),\n",
    "                       activation='linear',\n",
    "                       padding='valid',\n",
    "                       kernel_initializer='he_normal',\n",
    "                       kernel_regularizer=l2(weight_decay),\n",
    "                       name='score_{}'.format(block_name))(x)\n",
    "        if y is not None:\n",
    "            def scaling(xx, ss=1):\n",
    "                return xx * ss\n",
    "            scaled = Lambda(scaling, arguments={'ss': scale},\n",
    "                            name='scale_{}'.format(block_name))(score)\n",
    "            score = add([y, scaled])\n",
    "        upscore = BilinearUpSampling2D(\n",
    "            target_shape=target_shape,\n",
    "            name='upscore_{}'.format(block_name))(score)\n",
    "        return upscore\n",
    "    return f\n",
    "\n",
    "def vgg_score(crop_offset='centered'):\n",
    "    def f(x, y):\n",
    "        score = CroppingLike2D(target_shape=K.int_shape(\n",
    "            x), offset=crop_offset, name='score')(y)\n",
    "        return score\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:58.306776Z",
     "start_time": "2018-06-18T18:57:58.273821Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, inputs, blocks, \n",
    "                 weights=None, trainable=True,\n",
    "                 name='encoder'):\n",
    "        inverse_pyramid = []\n",
    "        \n",
    "        conv_blocks = blocks[:-1]\n",
    "        for i, block in enumerate(conv_blocks):\n",
    "            if i == 0:\n",
    "                x = block(inputs)\n",
    "                inverse_pyramid.append(x)\n",
    "            elif i < len(conv_blocks) - 1:\n",
    "                x = block(x)\n",
    "                inverse_pyramid.append(x)\n",
    "            else:\n",
    "                x = block(x)\n",
    "        \n",
    "        # fully convolutional block\n",
    "        fc_block = blocks[-1]\n",
    "        y = fc_block(x)\n",
    "        inverse_pyramid.append(y)\n",
    "        \n",
    "        # Reverse the pyramid features\n",
    "        outputs = list(reversed(inverse_pyramid))\n",
    "        \n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:59.360476Z",
     "start_time": "2018-06-18T18:57:59.351895Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def Decoder(pyramid, blocks):\n",
    "    if len(blocks) != len(pyramid):\n",
    "        raise ValueError('`blocks` needs to match the length of'\n",
    "                         '`pyramid`.')\n",
    "    \n",
    "    decoded = None\n",
    "    for feat, blk in zip(pyramid, blocks):\n",
    "        print(feat, decoded, blk)\n",
    "        decoded = blk(feat, decoded)\n",
    "    return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:59.713523Z",
     "start_time": "2018-06-18T18:57:59.709240Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(250, 250, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:57:59.885118Z",
     "start_time": "2018-06-18T18:57:59.878161Z"
    }
   },
   "outputs": [],
   "source": [
    "blocks = [vgg_conv(64, 1, block_name='block1'),\n",
    "          vgg_conv(128, 1, block_name='block2'),\n",
    "          vgg_conv(256, 1, block_name='block3'),\n",
    "          vgg_conv(512, 1, block_name='block4'),\n",
    "          vgg_conv(512, 1, block_name='block5'),\n",
    "          vgg_fc(4096)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:58:00.530180Z",
     "start_time": "2018-06-18T18:58:00.362966Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(inputs, blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:58:00.638781Z",
     "start_time": "2018-06-18T18:58:00.635308Z"
    }
   },
   "outputs": [],
   "source": [
    "feat_pyramid = encoder.outputs   # A feature pyramid with 5 scales\n",
    "feat_pyramid = feat_pyramid[:5]  # Select only the top three scale of the pyramid\n",
    "feat_pyramid.append(inputs)      # Add image to the bottom of the pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:58:00.946192Z",
     "start_time": "2018-06-18T18:58:00.939344Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'dropout_6/cond/Merge:0' shape=(?, 8, 8, 4096) dtype=float32>,\n",
       " <tf.Tensor 'dropout_4/cond/Merge:0' shape=(?, 16, 16, 512) dtype=float32>,\n",
       " <tf.Tensor 'dropout_3/cond/Merge:0' shape=(?, 32, 32, 256) dtype=float32>,\n",
       " <tf.Tensor 'dropout_2/cond/Merge:0' shape=(?, 63, 63, 128) dtype=float32>,\n",
       " <tf.Tensor 'dropout_1/cond/Merge:0' shape=(?, 125, 125, 64) dtype=float32>,\n",
       " <tf.Tensor 'input_4:0' shape=(?, 250, 250, 1) dtype=float32>]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:58:02.146459Z",
     "start_time": "2018-06-18T18:58:01.607463Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"dropout_6/cond/Merge:0\", shape=(?, 8, 8, 4096), dtype=float32) None <function vgg_deconv.<locals>.f at 0x1822d497b8>\n",
      "Tensor(\"dropout_4/cond/Merge:0\", shape=(?, 16, 16, 512), dtype=float32) Tensor(\"upsample_feat1/ResizeBilinear:0\", shape=(?, 16, 16, 512), dtype=float32) <function vgg_deconv.<locals>.f at 0x1822d49ea0>\n",
      "Tensor(\"dropout_3/cond/Merge:0\", shape=(?, 32, 32, 256), dtype=float32) Tensor(\"upsample_feat2/ResizeBilinear:0\", shape=(?, 32, 32, 256), dtype=float32) <function vgg_deconv.<locals>.f at 0x1823117b70>\n",
      "Tensor(\"dropout_2/cond/Merge:0\", shape=(?, 63, 63, 128), dtype=float32) Tensor(\"upsample_feat3/ResizeBilinear:0\", shape=(?, 63, 63, 128), dtype=float32) <function vgg_deconv.<locals>.f at 0x18231179d8>\n",
      "Tensor(\"dropout_1/cond/Merge:0\", shape=(?, 125, 125, 64), dtype=float32) Tensor(\"upsample_feat4/ResizeBilinear:0\", shape=(?, 125, 125, 124), dtype=float32) <function vgg_deconv.<locals>.f at 0x1823117ea0>\n"
     ]
    }
   ],
   "source": [
    "decode_blocks = [\n",
    "    vgg_deconv(classes=512, target_shape=(None, 16, 16, None), kernel_size=(4,4), strides=(2,2), block_name='feat1'),\n",
    "    vgg_deconv(classes=256, target_shape=(None, 32, 32, None), kernel_size=(4,4), strides=(2,2), block_name='feat2'),\n",
    "    vgg_deconv(classes=128, target_shape=(None, 63, 63, None), kernel_size=(4,4), strides=(2,2), block_name='feat3'),\n",
    "    vgg_deconv(classes=124, target_shape=(None, 125, 125, None), kernel_size=(4,4), strides=(2,2), block_name='feat4'),\n",
    "    vgg_deconv(classes=124, target_shape=(None, 250, 250, None), kernel_size=(16,16), strides=(2,2), block_name='feat5'),\n",
    "]\n",
    "\n",
    "outputs = Decoder(pyramid=feat_pyramid[:-1], blocks=decode_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:58:04.114525Z",
     "start_time": "2018-06-18T18:58:04.112022Z"
    }
   },
   "outputs": [],
   "source": [
    "# outputs = Activation('softmax')(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T18:58:42.653245Z",
     "start_time": "2018-06-18T18:58:41.609364Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "plot_model(model, show_shapes=True, to_file='orig_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:00:49.280256Z",
     "start_time": "2018-06-18T17:00:49.258275Z"
    }
   },
   "outputs": [],
   "source": [
    "adam = keras.optimizers.Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "\n",
    "def customLoss(target, output):\n",
    "    targets = tf.squeeze(K.cast(target, 'int64'), squeeze_dims=[3])\n",
    "    res = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=output)\n",
    "    return tf.reduce_mean(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T19:47:08.288259Z",
     "start_time": "2018-06-14T19:47:08.158425Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam,\n",
    "              loss=customLoss,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:38:30.465002Z",
     "start_time": "2018-06-18T17:38:30.407193Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:07.363790Z",
     "start_time": "2018-06-18T19:02:07.358551Z"
    }
   },
   "outputs": [],
   "source": [
    "input = Input(shape=(250, 250, 1))\n",
    "n_classes = 124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:07.700806Z",
     "start_time": "2018-06-18T19:02:07.518357Z"
    }
   },
   "outputs": [],
   "source": [
    "conv2 = Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu', name='conv2')(input)\n",
    "pool2 = MaxPooling2D(pool_size=(2,2), padding='same', name='pool2')(conv2)\n",
    "dropout2 = Dropout(rate=0.85, name='dropout2')(pool2)\n",
    "\n",
    "conv3 = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', name='conv3')(dropout2)\n",
    "pool3 = MaxPooling2D(pool_size=(2,2), padding='same', name='pool3')(conv3)\n",
    "dropout3 = Dropout(rate=0.85, name='dropout3')(pool3)\n",
    "\n",
    "conv4 = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', name='conv4')(dropout3)\n",
    "pool4 = MaxPooling2D(pool_size=(2,2), padding='same', name='pool4')(conv4)\n",
    "dropout4 = Dropout(rate=0.85, name='dropout4')(pool4)\n",
    "\n",
    "conv5 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='conv5')(dropout4)\n",
    "pool5 = MaxPooling2D(pool_size=(2,2), padding='same', name='pool5')(conv5)\n",
    "dropout5 = Dropout(rate=0.85, name='dropout5')(pool5)\n",
    "\n",
    "conv6 = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='conv6')(dropout5)\n",
    "pool6 = MaxPooling2D(pool_size=(2,2), padding='same', name='pool6')(conv6)\n",
    "dropout6 = Dropout(rate=0.85, name='dropout6')(pool6)\n",
    "\n",
    "conv7 = Conv2D(filters=4096, kernel_size=(3,3), padding='same', name='conv7')(dropout6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:08.045144Z",
     "start_time": "2018-06-18T19:02:07.702692Z"
    }
   },
   "outputs": [],
   "source": [
    "conv_t1 = Conv2DTranspose(filters=512, kernel_size=(4,4), strides=(2,2), padding='same', name='conv_t1')(conv7)\n",
    "conv_t1_up = BilinearUpSampling2D(target_size=tuple(pool5.get_shape().as_list()[1:3]))(conv_t1)\n",
    "\n",
    "stacked_1 = concatenate(inputs=[conv_t1_up, pool5], axis=-1, name='stacked_1')\n",
    "fuse_1_1 = Conv2D(filters=512, kernel_size=(1,1), activation='relu', padding='same', name='fuse_1_1')(stacked_1)\n",
    "fuse_1_2 = Conv2D(filters=512, kernel_size=(1,1), activation='relu', padding='same', name='fuse_1_2')(fuse_1_1)\n",
    "\n",
    "conv_t2 = Conv2DTranspose(filters=256, kernel_size=(4,4), strides=(2,2), padding='same', name='conv_t2')(fuse_1_2)\n",
    "conv_t2_up = BilinearUpSampling2D(target_size=tuple(pool4.get_shape().as_list()[1:3]))(conv_t2)\n",
    "\n",
    "stacked_2 = concatenate(inputs=[conv_t2_up, pool4], axis=-1, name='stacked_2')\n",
    "fuse_2_1 = Conv2D(filters=256, kernel_size=(1,1), activation='relu', padding='same', name='fuse_2_1')(stacked_2)\n",
    "fuse_2_2 = Conv2D(filters=256, kernel_size=(1,1), activation='relu', padding='same', name='fuse_2_2')(fuse_2_1)\n",
    "\n",
    "conv_t3 = Conv2DTranspose(filters=128, kernel_size=(4,4), strides=(2,2), padding='same', name='conv_t3')(fuse_2_2)\n",
    "conv_t3_up = BilinearUpSampling2D(target_size=tuple(pool3.get_shape().as_list()[1:3]))(conv_t3)\n",
    "\n",
    "stacked_3 = concatenate(inputs=[conv_t3_up, pool3], axis=-1, name='stacked_3')\n",
    "fuse_3_1 = Conv2D(filters=128, kernel_size=(1,1), activation='relu', padding='same', name='fuse_3_1')(stacked_3)\n",
    "fuse_3_2 = Conv2D(filters=128, kernel_size=(1,1), activation='relu', padding='same', name='fuse_3_2')(fuse_3_1)\n",
    "\n",
    "conv_t4 = Conv2DTranspose(filters=64, kernel_size=(4,4), strides=(2,2), padding='same', name='conv_t4')(fuse_3_2)\n",
    "conv_t4_up = BilinearUpSampling2D(target_size=tuple(pool2.get_shape().as_list()[1:3]))(conv_t4)\n",
    "\n",
    "stacked_4 = concatenate(inputs=[conv_t4_up, pool2], axis=-1, name='stacked_4')\n",
    "fuse_4_1 = Conv2D(filters=64, kernel_size=(1,1), activation='relu', padding='same', name='fuse_4_1')(stacked_4)\n",
    "fuse_4_2 = Conv2D(filters=64, kernel_size=(1,1), activation='relu', padding='same', name='fuse_4_2')(fuse_4_1)\n",
    "\n",
    "# Final upscaling\n",
    "output = Conv2DTranspose(filters=n_classes, kernel_size=(16,16), strides=(2,2), \n",
    "                         padding='same', name='output')(fuse_4_2)\n",
    "output_up = BilinearUpSampling2D(target_size=tuple(input.get_shape().as_list()[1:3]))(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:08.049743Z",
     "start_time": "2018-06-18T19:02:08.047422Z"
    }
   },
   "outputs": [],
   "source": [
    "# ?BilinearUpSampling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:08.699821Z",
     "start_time": "2018-06-18T19:02:08.695618Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(pool5.get_shape().as_list()[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:08.921072Z",
     "start_time": "2018-06-18T19:02:08.916494Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/teerapatjenrungrot/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"bi...)`\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "test_model = Model(input=input, output=output_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T19:02:11.229531Z",
     "start_time": "2018-06-18T19:02:09.572321Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "# test_model.summary()\n",
    "SVG(model_to_dot(test_model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "plot_model(test_model, show_shapes=True, to_file='model.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:06:40.837704Z",
     "start_time": "2018-06-18T17:06:39.954839Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class seg_dataset_reader:\n",
    "    path = \"\"\n",
    "    class_mappings = \"\"\n",
    "    files = []\n",
    "    images = []\n",
    "    annotations = []\n",
    "    test_images = []\n",
    "    test_annotations = []\n",
    "    batch_offset = 0\n",
    "    epochs_completed = 0\n",
    "\n",
    "    def __init__(self, deepscores_path, max_pages=40, crop=True, crop_size=[1000,1000], test_size=20):\n",
    "        \"\"\"\n",
    "        Initialize a file reader for the DeepScores classification data\n",
    "        :param records_list: path to the dataset\n",
    "        sample record: {'image': f, 'annotation': annotation_file, 'filename': filename}\n",
    "        \"\"\"\n",
    "        print(\"Initializing DeepScores Classification Batch Dataset Reader...\")\n",
    "        self.path = deepscores_path\n",
    "        self.max_pages = max_pages\n",
    "        self.crop = crop\n",
    "        self.crop_size = crop_size\n",
    "        self.test_size = test_size\n",
    "\n",
    "        images_list = []\n",
    "        images_glob = os.path.join(self.path, \"images_png\", '*.' + 'png')\n",
    "        images_list.extend(glob.glob(images_glob))\n",
    "\n",
    "        #shuffle image list\n",
    "        shuffle(images_list)\n",
    "\n",
    "        if max_pages is None:\n",
    "            max_pages = len(images_list)\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        if max_pages > len(images_list):\n",
    "            print(\"Not enough data, only \" + str(len(images_list)) + \" available\")\n",
    "\n",
    "        if test_size >= max_pages:\n",
    "            print(\"Test set too big (\"+str(test_size)+\"), max_pages is: \"+str(max_pages))\n",
    "            import sys\n",
    "            sys.exit(1)\n",
    "\n",
    "        print(\"Splitting dataset, train: \"+str(max_pages-test_size)+\" images, test: \"+str(test_size)+ \" images\")\n",
    "        test_image_list = images_list[0:test_size]\n",
    "        train_image_list = images_list[test_size:max_pages]\n",
    "\n",
    "        # test_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in test_image_list]\n",
    "        # train_annotation_list = [image_file.replace(\"/images_png/\", \"/pix_annotations_png/\") for image_file in train_image_list]\n",
    "\n",
    "        self._read_images(test_image_list,train_image_list)\n",
    "\n",
    "    def _read_images(self,test_image_list,train_image_list):\n",
    "\n",
    "        dat_train = [self._transform(filename) for filename in train_image_list]\n",
    "        for dat in dat_train:\n",
    "            self.images.append(dat[0])\n",
    "            self.annotations.append(dat[1])\n",
    "        self.images = np.array(self.images)\n",
    "        self.images = np.expand_dims(self.images, -1)\n",
    "\n",
    "        self.annotations = np.array(self.annotations)\n",
    "        self.annotations = np.expand_dims(self.annotations, -1)\n",
    "\n",
    "        print(\"Training set done\")\n",
    "        dat_test = [self._transform(filename) for filename in test_image_list]\n",
    "        for dat in dat_test:\n",
    "            self.test_images.append(dat[0])\n",
    "            self.test_annotations.append(dat[1])\n",
    "        self.test_images = np.array(self.test_images)\n",
    "        self.test_images = np.expand_dims(self.test_images, -1)\n",
    "\n",
    "        self.test_annotations = np.array(self.test_annotations)\n",
    "        self.test_annotations = np.expand_dims(self.test_annotations, -1)\n",
    "        print(\"Test set done\")\n",
    "\n",
    "\n",
    "    def _transform(self, filename):\n",
    "        image = misc.imread(filename)\n",
    "        annotation = misc.imread(filename.replace(\"/images_png/\", \"/pix_annotations_png/\"))\n",
    "        print(\"im working!\" + str(randint(0,10)))\n",
    "        if not image.shape[0:2] == annotation.shape[0:2]:\n",
    "            print(\"input and annotation have different sizes!\")\n",
    "            import sys\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "            sys.exit(1)\n",
    "\n",
    "        if image.shape[-1] != 1:\n",
    "            # take mean over color channels, image BW anyways --> fix in dataset creation\n",
    "            image = np.mean(image, -1)\n",
    "\n",
    "        if self.crop:\n",
    "            coord_0 = randint(0, (image.shape[0] - self.crop_size[0]))\n",
    "            coord_1 = randint(0, (image.shape[1] - self.crop_size[1]))\n",
    "\n",
    "            image = image[coord_0:(coord_0+self.crop_size[0]),coord_1:(coord_1+self.crop_size[1])]\n",
    "            annotation = annotation[coord_0:(coord_0 + self.crop_size[0]), coord_1:(coord_1 + self.crop_size[1])]\n",
    "\n",
    "        return [image, annotation]\n",
    "\n",
    "    # from PIL import Image\n",
    "    # im = Image.fromarray(image)\n",
    "    # im.show()\n",
    "    # im = Image.fromarray(annotation)\n",
    "    # im.show()\n",
    "\n",
    "\n",
    "    def get_records(self):\n",
    "        return self.images, self.annotations\n",
    "\n",
    "    def reset_batch_offset(self, offset=0):\n",
    "        self.batch_offset = offset\n",
    "\n",
    "    def get_test_records(self):\n",
    "        return self.test_images, self.test_annotations\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        start = self.batch_offset\n",
    "        self.batch_offset += batch_size\n",
    "        if self.batch_offset > self.images.shape[0]:\n",
    "            # Finished epoch\n",
    "            self.epochs_completed += 1\n",
    "            #             print(\"****************** Epochs completed: \" + str(self.epochs_completed) + \"******************\")\n",
    "            # Shuffle the data\n",
    "            perm = np.arange(self.images.shape[0])\n",
    "            np.random.shuffle(perm)\n",
    "            self.images = self.images[perm]\n",
    "            self.annotations = self.annotations[perm]\n",
    "            # Start next epoch\n",
    "            start = 0\n",
    "            self.batch_offset = batch_size\n",
    "\n",
    "        end = self.batch_offset\n",
    "        return self.images[start:end], self.annotations[start:end]\n",
    "\n",
    "    def get_random_batch(self, batch_size):\n",
    "        indexes = np.random.randint(0, self.images.shape[0], size=[batch_size]).tolist()\n",
    "        return self.images[indexes], self.annotations[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T19:47:25.195528Z",
     "start_time": "2018-06-14T19:47:08.879063Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_reader = seg_dataset_reader('/home/mirlab/Downloads/DeepScores/',\n",
    "                                 crop=True, crop_size=[250,250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T19:47:25.198921Z",
     "start_time": "2018-06-14T19:47:25.196831Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NUM_ITERATION = 10000\n",
    "PRINT_EVERY_ITR = 100\n",
    "SAVE_EVERY_ITR = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T19:47:25.253839Z",
     "start_time": "2018-06-14T19:47:25.200110Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hist = {\n",
    "    'acc': [],\n",
    "    'loss': [],\n",
    "    'val_acc': [],\n",
    "    'val_loss': [],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:10.117770Z",
     "start_time": "2018-06-14T19:47:25.256147Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for itr in range(NUM_ITERATION):\n",
    "    train_img, train_annotation = data_reader.next_batch(20)\n",
    "    val_img, val_annotation = data_reader.get_test_records()\n",
    "    history = model.fit(x=train_img, y=train_annotation, verbose=False, validation_data=(val_img, val_annotation))\n",
    "    for key in history.history:\n",
    "        value = history.history[key]\n",
    "        hist[key].append(value)\n",
    "        \n",
    "    if itr % PRINT_EVERY_ITR == 0:\n",
    "        plt.close()\n",
    "        print(\"Finish Iteration {:}\".format(itr))\n",
    "        plt.figure(figsize=(20,6))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot(hist['loss'], label='training')\n",
    "        plt.plot(hist['val_loss'], label='validation')\n",
    "        plt.title('Loss')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.xlabel('Epochs')\n",
    "\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(hist['acc'], label='training')\n",
    "        plt.plot(hist['val_acc'], label='validation')\n",
    "        plt.title('Accuracy')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epochs')\n",
    "\n",
    "        plt.show()\n",
    "    if itr % SAVE_EVERY_ITR == 0:\n",
    "        outputFile = 'model-checkpoint-iteration-{:}.h5'.format(itr)\n",
    "        print(\"Save model to {:}\".format(outputFile))\n",
    "        model.save(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T19:43:37.158769Z",
     "start_time": "2018-06-14T19:43:36.818624Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:01:13.319940Z",
     "start_time": "2018-06-18T17:01:06.418532Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('model-checkpoint-iteration-36000.h5', custom_objects={\n",
    "    'BilinearUpSampling2D': BilinearUpSampling2D,\n",
    "    'CroppingLike2D': CroppingLike2D,\n",
    "    'customLoss': customLoss,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:10:50.448443Z",
     "start_time": "2018-06-18T17:10:48.664894Z"
    }
   },
   "outputs": [],
   "source": [
    "data_reader = seg_dataset_reader('./data', max_pages=1, test_size=0,\n",
    "                                 crop=True, crop_size=[500,500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:10:52.527361Z",
     "start_time": "2018-06-18T17:10:52.518367Z"
    }
   },
   "outputs": [],
   "source": [
    "train_img, train_annotation = data_reader.next_batch(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:10:53.389389Z",
     "start_time": "2018-06-18T17:10:53.384994Z"
    }
   },
   "outputs": [],
   "source": [
    "train_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:11:13.932011Z",
     "start_time": "2018-06-18T17:10:56.632077Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(model.predict(train_img), axis=-1) # get the argmax of 124 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:11:32.406993Z",
     "start_time": "2018-06-18T17:11:32.404377Z"
    }
   },
   "outputs": [],
   "source": [
    "img_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:11:33.086647Z",
     "start_time": "2018-06-18T17:11:32.598091Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(train_img[img_id,:,:,0], cmap='gray')\n",
    "plt.title(\"Input image\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(train_annotation[img_id,:,:,0], cmap='gray')\n",
    "plt.title(\"Ground Truth annotation\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(prediction[img_id,:,:], cmap='gray')\n",
    "plt.title(\"Semantic Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:21:50.244128Z",
     "start_time": "2018-06-18T17:21:33.016596Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ypred = np.argmax(model.predict(train_img), axis=-1) # get the argmax of 124 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:23:39.751822Z",
     "start_time": "2018-06-18T17:23:35.483429Z"
    }
   },
   "outputs": [],
   "source": [
    "def myMetric(ytarget, ypred):\n",
    "    num = 0\n",
    "    denom = 0\n",
    "    for img in range(ytarget.shape[0]):\n",
    "        for row in range(ytarget.shape[1]):\n",
    "            for col in range(ytarget.shape[2]):\n",
    "                if ytarget[img,row,col,0] != 0 or ypred[img,row,col] != 0:\n",
    "                    denom += 1\n",
    "                    if ytarget[img,row,col,0] == ypred[img,row,col]:\n",
    "                        num += 1\n",
    "    return (num, denom)\n",
    "    \n",
    "myMetric(train_annotation, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-18T17:22:11.985074Z",
     "start_time": "2018-06-18T17:22:11.980913Z"
    }
   },
   "outputs": [],
   "source": [
    "train_annotation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:52.168996Z",
     "start_time": "2018-06-14T20:24:52.165454Z"
    }
   },
   "outputs": [],
   "source": [
    "test_img, test_annotation = data_reader.get_test_records()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:53.306662Z",
     "start_time": "2018-06-14T20:24:52.486524Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = np.argmax(model.predict(test_img), axis=-1) # get the argmax of 124 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:58.067287Z",
     "start_time": "2018-06-14T20:24:58.063643Z"
    }
   },
   "outputs": [],
   "source": [
    "img_id = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:58.648459Z",
     "start_time": "2018-06-14T20:24:58.359297Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,20))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_img[img_id,:,:,0], cmap='gray')\n",
    "plt.title(\"Input image\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(test_annotation[img_id,:,:,0], cmap='gray')\n",
    "plt.title(\"Ground Truth annotation\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(prediction[img_id,:,:], cmap='gray')\n",
    "plt.title(\"Semantic Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:58.697397Z",
     "start_time": "2018-06-14T20:24:58.681540Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T20:24:54.208064Z",
     "start_time": "2018-06-14T20:24:54.205200Z"
    }
   },
   "outputs": [],
   "source": [
    "np.unique(test_annotation[img_id,:,:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
